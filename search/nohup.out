INFO 02-13 17:09:01 __init__.py:183] Automatically detected platform cuda.
INFO 02-13 17:09:09 config.py:520] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.
INFO 02-13 17:09:09 config.py:1328] Defaulting to use mp for distributed inference
WARNING 02-13 17:09:09 arg_utils.py:1107] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 02-13 17:09:09 config.py:1483] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 02-13 17:09:09 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 02-13 17:09:10 multiproc_worker_utils.py:298] Reducing Torch parallelism from 12 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-13 17:09:10 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:10 multiproc_worker_utils.py:227] Worker ready; awaiting tasks
INFO 02-13 17:09:11 cuda.py:225] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:11 cuda.py:225] Using Flash Attention backend.
INFO 02-13 17:09:12 utils.py:938] Found nccl from library libnccl.so.2
INFO 02-13 17:09:12 pynccl.py:67] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:12 utils.py:938] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:12 pynccl.py:67] vLLM is using nccl==2.21.5
INFO 02-13 17:09:12 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:12 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 02-13 17:09:12 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_69d18ecf'), local_subscribe_port=46679, remote_subscribe_port=None)
INFO 02-13 17:09:12 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:12 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:12 weight_utils.py:251] Using model weights format ['*.safetensors']
INFO 02-13 17:09:12 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:12 weight_utils.py:296] No model.safetensors.index.json found in remote.
INFO 02-13 17:09:13 weight_utils.py:296] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:13 model_runner.py:1115] Loading model weights took 1.6918 GB
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.77it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.77it/s]

INFO 02-13 17:09:13 model_runner.py:1115] Loading model weights took 1.6918 GB
INFO 02-13 17:09:16 worker.py:266] Memory profiling takes 1.92 seconds
INFO 02-13 17:09:16 worker.py:266] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.80) = 63.40GiB
INFO 02-13 17:09:16 worker.py:266] model weights take 1.69GiB; non_torch_memory takes 0.30GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 60.02GiB.
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:16 worker.py:266] Memory profiling takes 1.93 seconds
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:16 worker.py:266] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.80) = 63.40GiB
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:16 worker.py:266] model weights take 1.69GiB; non_torch_memory takes 0.30GiB; PyTorch activation peak memory takes 0.09GiB; the rest of the memory reserved for KV Cache is 61.32GiB.
INFO 02-13 17:09:16 executor_base.py:108] # CUDA blocks: 280967, # CPU blocks: 18724
INFO 02-13 17:09:16 executor_base.py:113] Maximum concurrency for 131072 tokens per request: 34.30x
INFO 02-13 17:09:19 model_runner.py:1430] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s][1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:19 model_runner.py:1430] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:12,  2.82it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:11,  2.84it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:11,  2.88it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:10,  2.90it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:01<00:10,  2.90it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:10,  2.85it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:02<00:09,  2.86it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:02<00:09,  2.89it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:03<00:09,  2.88it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:03<00:08,  2.85it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:03<00:08,  2.89it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:04<00:07,  2.90it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:04<00:07,  2.93it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:04<00:07,  2.95it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:05<00:06,  2.94it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:05<00:06,  2.92it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:05<00:06,  2.90it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:06<00:05,  2.91it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:06<00:05,  2.91it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:06<00:05,  2.93it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:07<00:04,  2.93it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:07<00:04,  2.93it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:07<00:04,  2.93it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:08<00:03,  2.93it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:08<00:03,  2.89it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:08<00:03,  2.90it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:09<00:02,  2.88it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:09<00:02,  2.86it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:10<00:02,  2.87it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:10<00:01,  2.89it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:10<00:01,  2.91it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:11<00:01,  2.92it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:11<00:00,  2.91it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:11<00:00,  2.90it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:12<00:00,  2.30it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]
INFO 02-13 17:09:31 custom_all_reduce.py:224] Registering 1995 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:36 custom_all_reduce.py:224] Registering 1995 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1695650)[0;0m INFO 02-13 17:09:36 model_runner.py:1558] Graph capturing finished in 17 secs, took 0.21 GiB
INFO 02-13 17:09:36 model_runner.py:1558] Graph capturing finished in 17 secs, took 0.21 GiB
INFO 02-13 17:09:36 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 22.63 seconds
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]
Some weights of the model checkpoint at Qwen/Qwen2.5-Math-PRM-7B were not used when initializing Qwen2ForProcessRewardModel: {'lm_head.weight'}
- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/30 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
  3%|▎         | 1/30 [03:04<1:28:59, 184.11s/it]  7%|▋         | 2/30 [27:18<7:14:40, 931.43s/it] 10%|█         | 3/30 [38:32<6:06:06, 813.59s/it] 13%|█▎        | 4/30 [51:47<5:49:25, 806.35s/it] 17%|█▋        | 5/30 [1:13:45<6:52:54, 990.97s/it] 20%|██        | 6/30 [1:39:54<7:54:56, 1187.34s/it] 23%|██▎       | 7/30 [1:51:16<6:31:50, 1022.19s/it] 27%|██▋       | 8/30 [2:01:18<5:25:47, 888.53s/it]  30%|███       | 9/30 [2:07:22<4:13:31, 724.35s/it] 33%|███▎      | 10/30 [2:11:00<3:09:20, 568.04s/it] 37%|███▋      | 11/30 [2:33:16<4:14:22, 803.29s/it] 40%|████      | 12/30 [2:37:27<3:10:32, 635.13s/it] 43%|████▎     | 13/30 [2:56:26<3:43:12, 787.80s/it] 47%|████▋     | 14/30 [3:09:13<3:28:23, 781.50s/it] 50%|█████     | 15/30 [3:28:26<3:43:23, 893.57s/it] 53%|█████▎    | 16/30 [3:46:53<3:43:29, 957.80s/it] 57%|█████▋    | 17/30 [4:06:18<3:41:01, 1020.10s/it] 60%|██████    | 18/30 [4:23:10<3:23:31, 1017.64s/it] 63%|██████▎   | 19/30 [4:44:38<3:21:27, 1098.90s/it] 67%|██████▋   | 20/30 [4:56:41<2:44:18, 985.88s/it]  70%|███████   | 21/30 [5:17:38<2:40:06, 1067.38s/it] 73%|███████▎  | 22/30 [5:30:54<2:11:26, 985.81s/it]  77%|███████▋  | 23/30 [5:45:19<1:50:47, 949.69s/it] 80%|████████  | 24/30 [6:03:32<1:39:16, 992.78s/it] 83%|████████▎ | 25/30 [6:06:03<1:01:40, 740.10s/it] 87%|████████▋ | 26/30 [6:17:37<48:25, 726.41s/it]   90%|█████████ | 27/30 [6:32:28<38:46, 775.55s/it] 93%|█████████▎| 28/30 [6:56:36<32:35, 977.54s/it] 97%|█████████▋| 29/30 [7:14:27<16:45, 1005.42s/it]100%|██████████| 30/30 [7:22:40<00:00, 851.66s/it] 100%|██████████| 30/30 [7:22:40<00:00, 885.34s/it]
Accuracy: 10/30 (33.33%)
[rank0]:[W214 00:32:31.575953968 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO 02-14 03:50:57 __init__.py:183] Automatically detected platform cuda.
INFO 02-14 03:51:04 config.py:520] This model supports multiple tasks: {'score', 'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.
INFO 02-14 03:51:04 config.py:1328] Defaulting to use mp for distributed inference
WARNING 02-14 03:51:04 arg_utils.py:1107] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.
INFO 02-14 03:51:04 config.py:1483] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 02-14 03:51:04 llm_engine.py:232] Initializing an LLM engine (v0.7.0) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
WARNING 02-14 03:51:05 multiproc_worker_utils.py:298] Reducing Torch parallelism from 12 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 02-14 03:51:05 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:05 multiproc_worker_utils.py:227] Worker ready; awaiting tasks
INFO 02-14 03:51:06 cuda.py:225] Using Flash Attention backend.
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:06 cuda.py:225] Using Flash Attention backend.
INFO 02-14 03:51:07 utils.py:938] Found nccl from library libnccl.so.2
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:07 utils.py:938] Found nccl from library libnccl.so.2
INFO 02-14 03:51:07 pynccl.py:67] vLLM is using nccl==2.21.5
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:07 pynccl.py:67] vLLM is using nccl==2.21.5
INFO 02-14 03:51:07 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:07 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/ubuntu/.cache/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 02-14 03:51:07 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_0feeaad0'), local_subscribe_port=51647, remote_subscribe_port=None)
INFO 02-14 03:51:07 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:07 model_runner.py:1110] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B...
INFO 02-14 03:51:07 weight_utils.py:251] Using model weights format ['*.safetensors']
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:08 weight_utils.py:251] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:01<00:09,  1.34s/it]
Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:02<00:08,  1.40s/it]
Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:04<00:07,  1.41s/it]
Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:05<00:05,  1.41s/it]
Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:06<00:03,  1.13s/it]
Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:07<00:02,  1.17s/it]
Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:08<00:01,  1.26s/it]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:10<00:00,  1.31s/it]
Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:10<00:00,  1.29s/it]

[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:18 model_runner.py:1115] Loading model weights took 30.7293 GB
INFO 02-14 03:51:18 model_runner.py:1115] Loading model weights took 30.7293 GB
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:21 worker.py:266] Memory profiling takes 2.31 seconds
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:21 worker.py:266] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.80) = 63.40GiB
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:21 worker.py:266] model weights take 30.73GiB; non_torch_memory takes 0.30GiB; PyTorch activation peak memory takes 0.26GiB; the rest of the memory reserved for KV Cache is 32.11GiB.
INFO 02-14 03:51:21 worker.py:266] Memory profiling takes 2.34 seconds
INFO 02-14 03:51:21 worker.py:266] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.80) = 63.40GiB
INFO 02-14 03:51:21 worker.py:266] model weights take 30.73GiB; non_torch_memory takes 0.30GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 30.97GiB.
INFO 02-14 03:51:21 executor_base.py:108] # CUDA blocks: 15855, # CPU blocks: 2048
INFO 02-14 03:51:21 executor_base.py:113] Maximum concurrency for 131072 tokens per request: 1.94x
INFO 02-14 03:51:23 model_runner.py:1430] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s][1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:23 model_runner.py:1430] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:15,  2.24it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:14,  2.27it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:14,  2.28it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:13,  2.28it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:13,  2.28it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:12,  2.30it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:12,  2.31it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:11,  2.31it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:03<00:11,  2.31it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:10,  2.31it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:04<00:10,  2.32it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.29it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:05<00:09,  2.30it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.29it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:08,  2.27it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:06<00:08,  2.29it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:07,  2.32it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:07<00:07,  2.36it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:06,  2.39it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:08<00:06,  2.42it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:09<00:05,  2.40it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:09<00:05,  2.41it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:09<00:04,  2.42it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:10<00:04,  2.44it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:10<00:04,  2.44it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:11<00:03,  2.44it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:11<00:03,  2.41it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:11<00:02,  2.42it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:12<00:02,  2.43it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:12<00:02,  2.39it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:13<00:01,  2.40it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:13<00:01,  2.41it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:13<00:00,  2.43it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:14<00:00,  2.44it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.01it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.32it/s]
INFO 02-14 03:51:38 custom_all_reduce.py:224] Registering 4515 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:43 custom_all_reduce.py:224] Registering 4515 cuda graph addresses
[1;36m(VllmWorkerProcess pid=1709821)[0;0m INFO 02-14 03:51:43 model_runner.py:1558] Graph capturing finished in 20 secs, took 0.35 GiB
INFO 02-14 03:51:43 model_runner.py:1558] Graph capturing finished in 20 secs, took 0.35 GiB
INFO 02-14 03:51:43 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 24.52 seconds
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.22it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.18it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]
Some weights of the model checkpoint at Qwen/Qwen2.5-Math-PRM-7B were not used when initializing Qwen2ForProcessRewardModel: {'lm_head.weight'}
- This IS expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing Qwen2ForProcessRewardModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|          | 0/30 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [01:18<00:00, 78.66s/it, est. speed input: 1.96 toks/s, output: 36.82 toks/s][AProcessed prompts: 100%|██████████| 1/1 [01:18<00:00, 78.66s/it, est. speed input: 1.96 toks/s, output: 36.82 toks/s]
  3%|▎         | 1/30 [01:18<38:01, 78.66s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [07:13<00:00, 433.27s/it, est. speed input: 0.29 toks/s, output: 32.94 toks/s][AProcessed prompts: 100%|██████████| 1/1 [07:13<00:00, 433.27s/it, est. speed input: 0.29 toks/s, output: 32.94 toks/s]
  7%|▋         | 2/30 [08:31<2:14:03, 287.26s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:46<00:00, 1066.46s/it, est. speed input: 0.09 toks/s, output: 30.73 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:46<00:00, 1066.46s/it, est. speed input: 0.09 toks/s, output: 30.73 toks/s]
 10%|█         | 3/30 [26:18<4:49:22, 643.06s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:46<00:00, 1066.11s/it, est. speed input: 0.10 toks/s, output: 30.74 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:46<00:00, 1066.11s/it, est. speed input: 0.10 toks/s, output: 30.74 toks/s]
 13%|█▎        | 4/30 [44:04<5:51:01, 810.08s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [02:50<00:00, 170.71s/it, est. speed input: 0.50 toks/s, output: 36.80 toks/s][AProcessed prompts: 100%|██████████| 1/1 [02:50<00:00, 170.71s/it, est. speed input: 0.50 toks/s, output: 36.80 toks/s]
 17%|█▋        | 5/30 [46:55<4:01:27, 579.52s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [03:07<00:00, 187.48s/it, est. speed input: 0.92 toks/s, output: 36.71 toks/s][AProcessed prompts: 100%|██████████| 1/1 [03:07<00:00, 187.48s/it, est. speed input: 0.92 toks/s, output: 36.71 toks/s]
 20%|██        | 6/30 [50:02<2:58:29, 446.22s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [08:19<00:00, 499.89s/it, est. speed input: 0.23 toks/s, output: 32.25 toks/s][AProcessed prompts: 100%|██████████| 1/1 [08:19<00:00, 499.89s/it, est. speed input: 0.23 toks/s, output: 32.25 toks/s]
 23%|██▎       | 7/30 [58:22<2:57:46, 463.77s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [02:59<00:00, 179.49s/it, est. speed input: 0.39 toks/s, output: 36.81 toks/s][AProcessed prompts: 100%|██████████| 1/1 [02:59<00:00, 179.49s/it, est. speed input: 0.39 toks/s, output: 36.81 toks/s]
 27%|██▋       | 8/30 [1:01:22<2:16:51, 373.27s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [02:00<00:00, 120.21s/it, est. speed input: 0.99 toks/s, output: 36.95 toks/s][AProcessed prompts: 100%|██████████| 1/1 [02:00<00:00, 120.21s/it, est. speed input: 0.99 toks/s, output: 36.95 toks/s]
 30%|███       | 9/30 [1:03:22<1:42:57, 294.16s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [01:23<00:00, 83.66s/it, est. speed input: 1.73 toks/s, output: 37.08 toks/s][AProcessed prompts: 100%|██████████| 1/1 [01:23<00:00, 83.66s/it, est. speed input: 1.73 toks/s, output: 37.08 toks/s]
 33%|███▎      | 10/30 [1:04:45<1:16:23, 229.17s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:42<00:00, 1062.68s/it, est. speed input: 0.09 toks/s, output: 30.84 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:42<00:00, 1062.68s/it, est. speed input: 0.09 toks/s, output: 30.84 toks/s]
 37%|███▋      | 11/30 [1:22:28<2:33:21, 484.27s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [01:27<00:00, 87.59s/it, est. speed input: 0.86 toks/s, output: 37.07 toks/s][AProcessed prompts: 100%|██████████| 1/1 [01:27<00:00, 87.59s/it, est. speed input: 0.86 toks/s, output: 37.07 toks/s]
 40%|████      | 12/30 [1:23:56<1:49:04, 363.60s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [02:07<00:00, 127.33s/it, est. speed input: 0.53 toks/s, output: 36.97 toks/s][AProcessed prompts: 100%|██████████| 1/1 [02:07<00:00, 127.33s/it, est. speed input: 0.53 toks/s, output: 36.97 toks/s]
 43%|████▎     | 13/30 [1:26:03<1:22:44, 292.02s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:55<00:00, 1075.87s/it, est. speed input: 0.11 toks/s, output: 30.46 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:55<00:00, 1075.87s/it, est. speed input: 0.11 toks/s, output: 30.46 toks/s]
 47%|████▋     | 14/30 [1:43:59<2:21:00, 528.78s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [03:08<00:00, 188.61s/it, est. speed input: 0.56 toks/s, output: 36.76 toks/s][AProcessed prompts: 100%|██████████| 1/1 [03:08<00:00, 188.61s/it, est. speed input: 0.56 toks/s, output: 36.76 toks/s]
 50%|█████     | 15/30 [1:47:08<1:46:33, 426.25s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [01:38<00:00, 98.74s/it, est. speed input: 1.35 toks/s, output: 36.96 toks/s][AProcessed prompts: 100%|██████████| 1/1 [01:38<00:00, 98.74s/it, est. speed input: 1.35 toks/s, output: 36.96 toks/s]
 53%|█████▎    | 16/30 [1:48:46<1:16:27, 327.67s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [02:21<00:00, 141.16s/it, est. speed input: 0.52 toks/s, output: 36.91 toks/s][AProcessed prompts: 100%|██████████| 1/1 [02:21<00:00, 141.16s/it, est. speed input: 0.52 toks/s, output: 36.91 toks/s]
 57%|█████▋    | 17/30 [1:51:07<58:50, 271.58s/it]  
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:41<00:00, 1061.38s/it, est. speed input: 0.10 toks/s, output: 30.87 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:41<00:00, 1061.38s/it, est. speed input: 0.10 toks/s, output: 30.87 toks/s]
 60%|██████    | 18/30 [2:08:49<1:41:46, 508.91s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [02:03<00:00, 123.61s/it, est. speed input: 1.76 toks/s, output: 36.94 toks/s][AProcessed prompts: 100%|██████████| 1/1 [02:03<00:00, 123.61s/it, est. speed input: 1.76 toks/s, output: 36.94 toks/s]
 63%|██████▎   | 19/30 [2:10:52<1:12:05, 393.19s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [02:19<00:00, 139.30s/it, est. speed input: 0.58 toks/s, output: 36.89 toks/s][AProcessed prompts: 100%|██████████| 1/1 [02:19<00:00, 139.30s/it, est. speed input: 0.58 toks/s, output: 36.89 toks/s]
 67%|██████▋   | 20/30 [2:13:12<52:49, 316.96s/it]  
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:53<00:00, 1073.29s/it, est. speed input: 0.16 toks/s, output: 30.53 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:53<00:00, 1073.29s/it, est. speed input: 0.16 toks/s, output: 30.53 toks/s]
 70%|███████   | 21/30 [2:31:05<1:21:35, 543.99s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:47<00:00, 1067.83s/it, est. speed input: 0.19 toks/s, output: 30.69 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:47<00:00, 1067.83s/it, est. speed input: 0.19 toks/s, output: 30.69 toks/s]
 73%|███████▎  | 22/30 [2:48:53<1:33:29, 701.20s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [02:09<00:00, 129.38s/it, est. speed input: 0.75 toks/s, output: 36.92 toks/s][AProcessed prompts: 100%|██████████| 1/1 [02:09<00:00, 129.38s/it, est. speed input: 0.75 toks/s, output: 36.92 toks/s]
 77%|███████▋  | 23/30 [2:51:02<1:01:47, 529.61s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:34<00:00, 1054.08s/it, est. speed input: 0.15 toks/s, output: 31.09 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:34<00:00, 1054.09s/it, est. speed input: 0.15 toks/s, output: 31.09 toks/s]
 80%|████████  | 24/30 [3:08:36<1:08:41, 686.98s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [01:32<00:00, 92.72s/it, est. speed input: 1.77 toks/s, output: 37.05 toks/s][AProcessed prompts: 100%|██████████| 1/1 [01:32<00:00, 92.72s/it, est. speed input: 1.77 toks/s, output: 37.05 toks/s]
 83%|████████▎ | 25/30 [3:10:09<42:23, 508.68s/it]  
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:42<00:00, 1062.82s/it, est. speed input: 0.08 toks/s, output: 30.83 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:42<00:00, 1062.82s/it, est. speed input: 0.08 toks/s, output: 30.83 toks/s]
 87%|████████▋ | 26/30 [3:27:52<44:59, 674.94s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [01:23<00:00, 83.42s/it, est. speed input: 0.98 toks/s, output: 37.08 toks/s][AProcessed prompts: 100%|██████████| 1/1 [01:23<00:00, 83.42s/it, est. speed input: 0.98 toks/s, output: 37.08 toks/s]
 90%|█████████ | 27/30 [3:29:15<24:52, 497.47s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:43<00:00, 1063.36s/it, est. speed input: 0.09 toks/s, output: 30.82 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:43<00:00, 1063.36s/it, est. speed input: 0.09 toks/s, output: 30.82 toks/s]
 93%|█████████▎| 28/30 [3:46:59<22:14, 667.25s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:43<00:00, 1063.18s/it, est. speed input: 0.39 toks/s, output: 30.82 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:43<00:00, 1063.18s/it, est. speed input: 0.39 toks/s, output: 30.82 toks/s]
 97%|█████████▋| 29/30 [4:04:42<13:06, 786.03s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts: 100%|██████████| 1/1 [17:42<00:00, 1062.06s/it, est. speed input: 0.11 toks/s, output: 30.85 toks/s][AProcessed prompts: 100%|██████████| 1/1 [17:42<00:00, 1062.06s/it, est. speed input: 0.11 toks/s, output: 30.85 toks/s]
100%|██████████| 30/30 [4:22:24<00:00, 868.84s/it]100%|██████████| 30/30 [4:22:24<00:00, 524.81s/it]
Accuracy: 16/30 (53.33%)
  0%|          | 0/30 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
  3%|▎         | 1/30 [12:15<5:55:20, 735.18s/it]  7%|▋         | 2/30 [1:21:47<21:26:29, 2756.76s/it] 10%|█         | 3/30 [2:11:31<21:27:15, 2860.59s/it] 13%|█▎        | 4/30 [3:05:51<21:47:55, 3018.30s/it] 17%|█▋        | 5/30 [3:51:22<20:14:29, 2914.78s/it] 20%|██        | 6/30 [4:47:29<20:27:21, 3068.38s/it] 23%|██▎       | 7/30 [5:41:58<20:01:23, 3134.08s/it] 27%|██▋       | 8/30 [6:07:14<16:00:15, 2618.87s/it] 30%|███       | 9/30 [6:23:33<12:17:16, 2106.48s/it] 33%|███▎      | 10/30 [6:32:48<9:02:29, 1627.47s/it] 37%|███▋      | 11/30 [7:34:18<11:55:13, 2258.62s/it] 40%|████      | 12/30 [7:55:29<9:47:29, 1958.31s/it]  43%|████▎     | 13/30 [8:11:33<7:49:30, 1657.08s/it] 47%|████▋     | 14/30 [8:53:53<8:32:59, 1923.72s/it] 50%|█████     | 15/30 [9:39:48<9:03:31, 2174.09s/it] 53%|█████▎    | 16/30 [10:04:50<7:40:07, 1971.96s/it] 57%|█████▋    | 17/30 [10:26:58<6:25:17, 1778.27s/it] 60%|██████    | 18/30 [11:22:27<7:28:49, 2244.16s/it] 63%|██████▎   | 19/30 [11:49:53<6:18:31, 2064.68s/it] 67%|██████▋   | 20/30 [12:14:36<5:14:58, 1889.89s/it] 70%|███████   | 21/30 [13:14:58<6:01:29, 2409.94s/it] 73%|███████▎  | 22/30 [14:17:37<6:15:17, 2814.66s/it] 77%|███████▋  | 23/30 [14:52:30<5:03:07, 2598.28s/it] 80%|████████  | 24/30 [15:28:27<4:06:35, 2465.87s/it] 83%|████████▎ | 25/30 [15:42:09<2:44:23, 1972.60s/it] 87%|████████▋ | 26/30 [16:26:07<2:24:49, 2172.36s/it] 90%|█████████ | 27/30 [16:40:24<1:28:52, 1777.64s/it] 93%|█████████▎| 28/30 [17:19:55<1:05:11, 1955.73s/it] 97%|█████████▋| 29/30 [18:16:16<39:43, 2383.20s/it]  100%|██████████| 30/30 [19:26:05<00:00, 2924.96s/it]100%|██████████| 30/30 [19:26:05<00:00, 2332.19s/it]
Accuracy: 20/30 (66.67%)
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [07:37<3:41:08, 457.55s/it]  7%|▋         | 2/30 [1:17:26<20:37:44, 2652.32s/it] 10%|█         | 3/30 [2:08:47<21:21:47, 2848.41s/it] 13%|█▎        | 4/30 [3:03:33<21:49:05, 3020.99s/it] 17%|█▋        | 5/30 [3:42:20<19:14:28, 2770.73s/it] 20%|██        | 6/30 [4:22:20<17:37:49, 2644.54s/it] 23%|██▎       | 7/30 [4:52:25<15:08:36, 2370.29s/it] 27%|██▋       | 8/30 [5:10:06<11:56:17, 1953.54s/it] 30%|███       | 9/30 [5:27:01<9:40:57, 1659.87s/it]  33%|███▎      | 10/30 [5:38:10<7:31:22, 1354.13s/it] 37%|███▋      | 11/30 [6:36:16<10:35:26, 2006.65s/it] 40%|████      | 12/30 [6:54:39<8:39:32, 1731.78s/it]  43%|████▎     | 13/30 [7:09:48<7:00:00, 1482.36s/it] 47%|████▋     | 14/30 [7:58:10<8:29:39, 1911.19s/it] 50%|█████     | 15/30 [8:39:17<8:39:43, 2078.88s/it] 53%|█████▎    | 16/30 [8:58:28<6:59:51, 1799.38s/it] 57%|█████▋    | 17/30 [9:37:18<7:04:28, 1959.13s/it] 60%|██████    | 18/30 [10:26:18<7:30:44, 2253.70s/it] 63%|██████▎   | 19/30 [11:05:03<6:57:08, 2275.29s/it] 67%|██████▋   | 20/30 [11:26:44<5:30:27, 1982.79s/it] 70%|███████   | 21/30 [12:30:44<6:21:00, 2540.09s/it] 73%|███████▎  | 22/30 [13:32:24<6:25:04, 2888.09s/it] 77%|███████▋  | 23/30 [14:16:54<5:29:19, 2822.83s/it] 80%|████████  | 24/30 [14:46:26<4:10:45, 2507.60s/it] 83%|████████▎ | 25/30 [14:58:53<2:44:56, 1979.33s/it] 87%|████████▋ | 26/30 [15:36:17<2:17:14, 2058.66s/it] 90%|█████████ | 27/30 [15:55:10<1:29:02, 1780.92s/it] 93%|█████████▎| 28/30 [16:42:56<1:10:12, 2106.44s/it] 97%|█████████▋| 29/30 [17:45:17<43:16, 2596.82s/it]  100%|██████████| 30/30 [18:49:27<00:00, 2972.71s/it]100%|██████████| 30/30 [18:49:27<00:00, 2258.91s/it]
Accuracy: 21/30 (70.00%)
[rank0]:[W215 22:29:50.560755551 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
